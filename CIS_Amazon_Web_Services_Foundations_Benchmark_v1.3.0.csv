section ,recommendation ,title,status,scoring status,description,rationale statement,impact statement,remediation procedure,audit procedure,additional information,CIS Controls,CCE-ID,references,,,,,,,,,
1,,Identity and Access Management,published,,This section contains recommendations for configuring identity and access management related options.,,,,,,,,,,,,,,,,,
1,1.6,"Ensure hardware MFA is enabled for the ""root user"" account",published,Automated,"The root user account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password as well as for an authentication code from their AWS MFA device. For Level 2, it is recommended that the root user account be protected with a hardware MFA.","A hardware MFA has a smaller attack surface than a virtual MFA. For example, a hardware MFA does not suffer the attack surface introduced by the mobile smartphone on which a virtual MFA resides.

**Note**: Using hardware MFA for many, many AWS accounts may create a logistical device management issue. If this is the case, consider implementing this Level 2 recommendation selectively to the highest security AWS accounts and the Level 1 recommendation applied to the remaining accounts.","Perform the following to establish a hardware MFA for the root user account:

1. Sign in to the AWS Management Console and open the IAM console at [https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/).
Note: to manage MFA devices for the AWS root user account, you must use your root account credentials to sign in to AWS. You cannot manage MFA devices for the root account using other credentials.
2. Choose `Dashboard` , and under `Security Status` , expand `Activate MFA` on your root account.
3. Choose `Activate MFA` 
4. In the wizard, choose `A hardware MFA` device and then choose `Next Step` .
5. In the `Serial Number` box, enter the serial number that is found on the back of the MFA device.
6. In the `Authentication Code 1` box, enter the six-digit number displayed by the MFA device. You might need to press the button on the front of the device to display the number.
7. Wait 30 seconds while the device refreshes the code, and then enter the next six-digit number into the `Authentication Code 2` box. You might need to press the button on the front of the device again to display the second number.
8. Choose `Next Step` . The MFA device is now associated with the AWS account. The next time you use your AWS account credentials to sign in, you must type a code from the hardware MFA device.

Remediation for this recommendation is not available through AWS CLI.","Perform the following to determine if the root user account has a hardware MFA setup:

1. Run the following command to determine if the root account has MFA setup:
```
 aws iam get-account-summary | grep ""AccountMFAEnabled""
```

The `AccountMFAEnabled` property is set to `1` will ensure that the root user account has MFA (Virtual or Hardware) Enabled.
If `AccountMFAEnabled` property is set to `0` the account is not compliant with this recommendation.

2. If `AccountMFAEnabled` property is set to `1`, determine root account has Hardware MFA enabled.
Run the following command to list all virtual MFA devices:
```
 aws iam list-virtual-mfa-devices 
```
If the output contains one MFA with the following Serial Number, it means the MFA is virtual, not hardware and the account is not compliant with this recommendation:

 `""SerialNumber"": ""arn:aws:iam::_<aws_account_number>_:mfa/root-account-mfa-device""`",,"IAM User account ""root"" for us-gov cloud regions does not have console access. This control is not applicable for us-gov cloud regions.",TITLE:Use Multifactor Authentication For All Administrative Access CONTROL:v7 4.5 DESCRIPTION:Use multi-factor authentication and encrypted channels for all administrative account access.;,CCE-78911-5,https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_virtual.html:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_physical.html#enable-hw-mfa-for-root,,,,,,,,,
1,1.18,Ensure IAM instance roles are used for AWS resource access from instances,published,Manual,"AWS access from within AWS instances can be done by either encoding AWS keys into AWS API calls or by assigning the instance to a role which has an appropriate permissions policy for the required access. ""AWS Access"" means accessing the APIs of AWS in order to access AWS resources or manage AWS account resources.","AWS IAM roles reduce the risks associated with sharing and rotating credentials that can be used outside of AWS itself. If credentials are compromised, they can be used from outside of the the AWS account they give access to. In contrast, in order to leverage role permissions an attacker would need to gain and maintain access to a specific instance to use the privileges associated with it.

Additionally, if credentials are encoded into compiled applications or other hard to change mechanisms, then they are even more unlikely to be properly rotated due to service disruption risks. As time goes on, credentials that cannot be rotated are more likely to be known by an increasing number of individuals who no longer work for the organization owning the credentials.","IAM roles can only be associated at the launch of an instance. To remediate an instance to add it to a role you must create a new instance.

If the instance has no external dependencies on its current private ip or public addresses are elastic IPs:

1. In AWS IAM create a new role. Assign a permissions policy if needed permissions are already known.
2. In the AWS console launch a new instance with identical settings to the existing instance, and ensure that the newly created role is selected.
3. Shutdown both the existing instance and the new instance.
4. Detach disks from both instances.
5. Attach the existing instance disks to the new instance.
6. Boot the new instance and you should have the same machine, but with the associated role.

**Note:** if your environment has dependencies on a dynamically assigned PRIVATE IP address you can create an AMI from the existing instance, destroy the old one and then when launching from the AMI, manually assign the previous private IP address.

**Note: **if your environment has dependencies on a dynamically assigned PUBLIC IP address there is not a way ensure the address is retained and assign an instance role. Dependencies on dynamically assigned public IP addresses are a bad practice and, if possible, you may wish to rebuild the instance with a new elastic IP address and make the investment to remediate affected systems while assigning the system to a role.","Whether an Instance Is Associated With a Role

For instances that are known to perform AWS actions, ensure that they belong to an instance role that has the necessary permissions:

1. Login to AWS Console (with appropriate permissions to View Identity Access Management Account Settings)
2. Open the EC2 Dashboard and choose ""Instances""
3. Click the EC2 instance that performs AWS actions, in the lower pane details find ""IAM Role""
4. If the Role is blank, the instance is not assigned to one.
5. If the Role is filled in, it does not mean the instance might not \*also\* have credentials encoded on it for some activities.

Whether an Instance Contains Embedded Credentials

On the instance that is known to perform AWS actions, audit all scripts and environment variables to ensure that none of them contain AWS credentials.

Whether an Instance Application Contains Embedded Credentials

Applications that run on an instance may also have credentials embedded. This is a bad practice, but even worse if the source code is stored in a public code repository such as github. When an application contains credentials can be determined by eliminating all other sources of credentials and if the application can still access AWS resources - it likely contains embedded credentials. Another method is to examine all source code and configuration files of the application.",,,TITLE:Incident Response and Management CONTROL:v7 19 DESCRIPTION:Incident Response and Management;,,https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html,,,,,,,,,
1,1.22,Ensure IAM users are managed centrally via identity federation or AWS Organizations for multi-account environments,published,Manual,"In multi-account environments, IAM user centralization facilitates greater user control. User access beyond the initial account is then provide via role assumption. Centralization of users can be accomplished through federation with an external identity provider or through the use of AWS Organizations.",Centralizing IAM user management to a single identity store reduces complexity and thus the likelihood of access management errors.,"The remediation procedure will vary based on the individual organization's implementation of identity federation and/or AWS Organizations with the acceptance criteria that no non-service IAM users, and non-root accounts, are present outside the account providing centralized IAM user management.","For multi-account AWS environments with an external identity provider... 

1. Determine the master account for identity federation or IAM user management
2. Login to that account through the AWS Management Console
3. Click `Services` 
4. Click `IAM` 
5. Click `Identity providers`
6. Verify the configuration

Then..., determine all accounts that should not have local users present. For each account...

1. Determine all accounts that should not have local users present
2. Log into the AWS Management Console
3. Switch role into each identified account
4. Click `Services` 
5. Click `IAM` 
6. Click `Users`
7. Confirm that no IAM users representing individuals are present

For multi-account AWS environments implementing AWS Organizations without an external identity provider... 

1. Determine all accounts that should not have local users present
2. Log into the AWS Management Console
3. Switch role into each identified account
4. Click `Services` 
5. Click `IAM` 
6. Click `Users`
7. Confirm that no IAM users representing individuals are present",,,"TITLE:Configure Centralized Point of Authentication CONTROL:v7 16.2 DESCRIPTION:Configure access for all accounts through as few centralized points of authentication as possible, including network, security, and cloud systems.;",,,,,,,,,,,
2,,Storage,published,,,,,,,,,,,,,,,,,,,
2.1,,Simple Storage Service (S3),published,,,,,,,,,,,,,,,,,,,
2.1,2.1.1,Ensure all S3 buckets employ encryption-at-rest,published,Manual,"Amazon S3 provides a variety of no, or low, cost encryption options to protect data at rest.",Encrypting data at rest reduces the likelihood that it is unintentionally exposed and can nullify the impact of disclosure if the encryption remains unbroken.,"**From Console:**

1. Login to AWS Management Console and open the Amazon S3 console using https://console.aws.amazon.com/s3/ 
2. Select the Check box next to the Bucket.
3. Click on 'Properties'.
4. Click on `Default Encryption`.
5. Select either `AES-256` or `AWS-KMS`
6. Click `Save`
7. Repeat for all the buckets in your AWS account lacking encryption.

**From Command Line:**

Run either 
```
aws s3api put-bucket-encryption --bucket <bucket name> --server-side-encryption-configuration '{""Rules"": [{""ApplyServerSideEncryptionByDefault"": {""SSEAlgorithm"": ""AES256""}}]}'
```
 or 
```
aws s3api put-bucket-encryption --bucket <bucket name> --server-side-encryption-configuration '{""Rules"": [{""ApplyServerSideEncryptionByDefault"": {""SSEAlgorithm"": ""aws:kms"",""KMSMasterKeyID"": ""aws/s3""}}]}'
```

**Note:** the KMSMasterKeyID can be set to the master key of your choosing; aws/s3 is an AWS preconfigured default.","**From Console:**

1. Login to AWS Management Console and open the Amazon S3 console using https://console.aws.amazon.com/s3/ 
2. Select the Check box next to the Bucket.
3. Click on 'Properties'.
4. Verify that `Default Encryption` displays either `AES-256` or `AWS-KMS`.
5. Repeat for all the buckets in your AWS account.

**From Command Line:**

1. Run 
```
aws s3 ls
```
2. For each bucket, run 
```
aws s3api get-bucket-encryption --bucket <bucket name>
```
3. Verify that either 
```
""SSEAlgorithm"": ""AES256""
```
 or 
```
""SSEAlgorithm"": ""aws:kms""```
 is displayed.",Amazon S3 buckets with default bucket encryption using SSE-KMS cannot be used as destination buckets for Amazon S3 server access logging. Only SSE-S3 default encryption is supported for server access log destination buckets.,S3 bucket encryption only applies to objects as they are placed in the bucket. Enabling S3 bucket encryption does **not** encrypt objects previously stored within the bucket.,"TITLE:Encrypt Sensitive Information at Rest CONTROL:v7 14.8 DESCRIPTION:Encrypt all sensitive information at rest using a tool that requires a secondary authentication mechanism not integrated into the operating system, in order to access the information.;",,https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html:https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html#bucket-encryption-related-resources,,,,,,,,,
2.1,2.1.2,Ensure S3 Bucket Policy allows HTTPS requests,published,Manual,"At the Amazon S3 bucket level, you can configure permissions through a bucket policy making the objects accessible only through HTTPS.","By default, Amazon S3 allows both HTTP and HTTPS requests. To achieve only allowing access to Amazon S3 objects through HTTPS you also have to explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests will not comply with this recommendation.","**From Console:**

1. Login to AWS Management Console and open the Amazon S3 console using https://console.aws.amazon.com/s3/
2. Select the Check box next to the Bucket.
3. Click on 'Permissions'.
4. Click 'Bucket Policy'
5. Add this to the existing policy filling in the required information
```
{
 ""Sid"": <optional>"",
 ""Effect"": ""Deny"",
 ""Principal"": ""*"",
 ""Action"": ""s3:GetObject"",
 ""Resource"": ""arn:aws:s3:::<bucket_name>/*"",
 ""Condition"": {
 ""Bool"": {
 ""aws:SecureTransport"": ""false""
 }
 }
 }
```
6. Save
7. Repeat for all the buckets in your AWS account that contain sensitive data.

**From Console** 

using AWS Policy Generator:

1. Repeat steps 1-4 above.
2. Click on `Policy Generator` at the bottom of the Bucket Policy Editor
3. Select Policy Type
`S3 Bucket Policy`
4. Add Statements
`Effect` = Deny
`Principal` = *
`AWS Service` = Amazon S3
`Actions` = GetObject
`Amazon Resource Name` = <ARN of the S3 Bucket>
5. Generate Policy
6. Copy the text and add it to the Bucket Policy.

**From Command Line:**

1. Export the bucket policy to a json file.
```
aws s3api get-bucket-policy --bucket <bucket_name> --query Policy --output text > policy.json
```

2. Modify the policy.json file by adding in this statement:
```
{
 ""Sid"": <optional>"",
 ""Effect"": ""Deny"",
 ""Principal"": ""*"",
 ""Action"": ""s3:GetObject"",
 ""Resource"": ""arn:aws:s3:::<bucket_name>/*"",
 ""Condition"": {
 ""Bool"": {
 ""aws:SecureTransport"": ""false""
 }
 }
 }
```
3. Apply this modified policy back to the S3 bucket:
```
aws s3api put-bucket-policy --bucket <bucket_name> --policy file://policy.json
```","To allow access to HTTPS you can use a condition that checks for the key `""aws:SecureTransport: true""`. This means that the request is sent through HTTPS but that HTTP can still be used. So to make sure you do not allow HTTP access confirm that there is a bucket policy that explicitly denies access for HTTP requests and that it contains the key ""aws:SecureTransport"": ""false"".

**From Console:**

1. Login to AWS Management Console and open the Amazon S3 console using https://console.aws.amazon.com/s3/
2. Select the Check box next to the Bucket.
3. Click on 'Permissions', then Click on `Bucket Policy`.
4. Ensure that a policy is listed that matches:
```
'{
 ""Sid"": <optional>,
 ""Effect"": ""Deny"",
 ""Principal"": ""*"",
 ""Action"": ""s3:GetObject"",
 ""Resource"": ""arn:aws:s3:::<bucket_name>/*"",
 ""Condition"": {
 ""Bool"": {
 ""aws:SecureTransport"": ""false""
 }'
```
`<optional>` and `<bucket_name>` will be specific to your account

5. Repeat for all the buckets in your AWS account.

**From Command Line:**

1. List all of the S3 Buckets 
```
aws s3 ls
```
2. Using the list of buckets run this command on each of them:
```
aws s3api get-bucket-policy --bucket <bucket_name> | grep aws:SecureTransport
```
3. Confirm that `aws:SecureTransport` is set to false `aws:SecureTransport:false`
4. Confirm that the policy line has Effect set to Deny 'Effect:Deny'",,,TITLE:Encrypt All Sensitive Information in Transit CONTROL:v7 14.4 DESCRIPTION:Encrypt all sensitive information in transit.;,,https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/:https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/:https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-bucket-policy.html,,,,,,,,,
2.2,,Elastic Compute Cloud (EC2),published,,,,,,,,,,,,,,,,,,,
2.2,2.2.1,Ensure EBS volume encryption is enabled,published,Manual,"Elastic Compute Cloud (EC2) supports encryption at rest when using the Elastic Block Store (EBS) service. While disabled by default, forcing encryption at EBS volume creation is supported.",Encrypting data at rest reduces the likelihood that it is unintentionally exposed and can nullify the impact of disclosure if the encryption remains unbroken.,"**From Console:**

1. Login to AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/ 
2. Under `Account attributes`, click `EBS encryption`.
3. Click `Manage`.
4. Click the `Enable` checkbox.
5. Click `Update EBS encryption`
6. Repeat for every region requiring the change.

**Note:** EBS volume encryption is configured per region.

**From Command Line:**

1. Run 
```
aws --region <region> ec2 enable-ebs-encryption-by-default
```
2. Verify that `""EbsEncryptionByDefault"": true` is displayed.
3. Repeat every region requiring the change.

**Note:** EBS volume encryption is configured per region.","**From Console:**

1. Login to AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/ 
2. Under `Account attributes`, click `EBS encryption`.
3. Verify `Always encrypt new EBS volumes` displays `Enabled`.
4. Review every region in-use.

**Note:** EBS volume encryption is configured per region.

**From Command Line:**

1. Run 
```
aws --region <region> ec2 get-ebs-encryption-by-default
```
2. Verify that `""EbsEncryptionByDefault"": true` is displayed.
3. Review every region in-use.

**Note:** EBS volume encryption is configured per region.",,Default EBS volume encryption only applies to newly created EBS volumes. Existing EBS volumes are **not** converted automatically.,"TITLE:Encrypt Sensitive Information at Rest CONTROL:v7 14.8 DESCRIPTION:Encrypt all sensitive information at rest using a tool that requires a secondary authentication mechanism not integrated into the operating system, in order to access the information.;",,https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html:https://aws.amazon.com/blogs/aws/new-opt-in-to-default-encryption-for-new-ebs-volumes/,,,,,,,,,
3,,Logging,published,,This section contains recommendations for configuring AWS's account logging features.,,,,,,,,,,,,,,,,,
3,3.2,Ensure CloudTrail log file validation is enabled,published,Automated,"CloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails.",Enabling log file validation will provide additional integrity checking of CloudTrail logs.,"Perform the following to enable log file validation on a given trail:

**From Console:**

1. Sign in to the AWS Management Console and open the IAM console at [https://console.aws.amazon.com/cloudtrail](https://console.aws.amazon.com/cloudtrail)
2. Click on `Trails` on the left navigation pane
3. Click on target trail
4. Within the `S3` section click on the edit icon (pencil)
5. Click `Advanced` 
6. Click on the `Yes` radio button in section `Enable log file validation` 
7. Click `Save` 

**From Command Line:**
```
aws cloudtrail update-trail --name <trail_name> --enable-log-file-validation
```
Note that periodic validation of logs using these digests can be performed by running the following command:
```
aws cloudtrail validate-logs --trail-arn <trail_arn> --start-time <start_time> --end-time <end_time>
```","Perform the following on each trail to determine if log file validation is enabled:

**From Console:**

1. Sign in to the AWS Management Console and open the IAM console at [https://console.aws.amazon.com/cloudtrail](https://console.aws.amazon.com/cloudtrail)
2. Click on `Trails` on the left navigation pane
3. For Every Trail:
- Click on a trail via the link in the _Name_ column
- Under the `S3` section, ensure `Enable log file validation` is set to `Yes` 

**From Command Line:**
```
aws cloudtrail describe-trails
```
Ensure `LogFileValidationEnabled` is set to `true` for each trail",,,"TITLE:Maintenance, Monitoring and Analysis of Audit Logs CONTROL:v7 6 DESCRIPTION:Maintenance, Monitoring and Analysis of Audit Logs;",CCE-78914-9,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-enabling.html,,,,,,,,,
3,3.7,Ensure CloudTrail logs are encrypted at rest using KMS CMKs,published,Automated,"AWS CloudTrail is a web service that records AWS API calls for an account and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data, and uses Hardware Security Modules (HSMs) to protect the security of encryption keys. CloudTrail logs can be configured to leverage server side encryption (SSE) and KMS customer created master keys (CMK) to further protect CloudTrail logs. It is recommended that CloudTrail be configured to use SSE-KMS.",Configuring CloudTrail to use SSE-KMS provides additional confidentiality controls on log data as a given user must have S3 read permission on the corresponding log bucket and must be granted decrypt permission by the CMK policy.,"Perform the following to configure CloudTrail to use SSE-KMS:

**From Console:**

1. Sign in to the AWS Management Console and open the CloudTrail console at [https://console.aws.amazon.com/cloudtrail](https://console.aws.amazon.com/cloudtrail)
2. In the left navigation pane, choose `Trails` .
3. Click on a Trail
4. Under the `S3` section click on the edit button (pencil icon)
5. Click `Advanced` 
6. Select an existing CMK from the `KMS key Id` drop-down menu
 - Note: Ensure the CMK is located in the same region as the S3 bucket
 - Note: You will need to apply a KMS Key policy on the selected CMK in order for CloudTrail as a service to encrypt and decrypt log files using the CMK provided. Steps are provided [here](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-kms-key-policy-for-cloudtrail.html) for editing the selected CMK Key policy
7. Click `Save` 
8. You will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files.
9. Click `Yes` 

**From Command Line:**
```
aws cloudtrail update-trail --name <trail_name> --kms-id <cloudtrail_kms_key>
aws kms put-key-policy --key-id <cloudtrail_kms_key> --policy <cloudtrail_kms_key_policy>
```","Perform the following to determine if CloudTrail is configured to use SSE-KMS:

**From Console:**

1. Sign in to the AWS Management Console and open the CloudTrail console at [https://console.aws.amazon.com/cloudtrail](https://console.aws.amazon.com/cloudtrail)
2. In the left navigation pane, choose `Trails` .
3. Select a Trail
4. Under the `S3` section, ensure `Encrypt log files` is set to `Yes` and a KMS key ID is specified in the `KSM Key Id` field.

**From Command Line:**

1. Run the following command:
```
 aws cloudtrail describe-trails 
```
2. For each trail listed, SSE-KMS is enabled if the trail has a `KmsKeyId` property defined.",Customer created keys incur an additional cost. See https://aws.amazon.com/kms/pricing/ for more information.,"3 statements which need to be added to the CMK policy:

1\. Enable Cloudtrail to describe CMK properties
```
<pre class=""programlisting"" style=""font-style: normal;"">{
 ""Sid"": ""Allow CloudTrail access"",
 ""Effect"": ""Allow"",
 ""Principal"": {
 ""Service"": ""cloudtrail.amazonaws.com""
 },
 ""Action"": ""kms:DescribeKey"",
 ""Resource"": ""*""
}
```
2\. Granting encrypt permissions
```
<pre class=""programlisting"" style=""font-style: normal;"">{
 ""Sid"": ""Allow CloudTrail to encrypt logs"",
 ""Effect"": ""Allow"",
 ""Principal"": {
 ""Service"": ""cloudtrail.amazonaws.com""
 },
 ""Action"": ""kms:GenerateDataKey*"",
 ""Resource"": ""*"",
 ""Condition"": {
 ""StringLike"": {
 ""kms:EncryptionContext:aws:cloudtrail:arn"": [
 ""arn:aws:cloudtrail:*:aws-account-id:trail/*""
 ]
 }
 }
}
```
3\. Granting decrypt permissions
```
<pre class=""programlisting"" style=""font-style: normal;"">{
 ""Sid"": ""Enable CloudTrail log decrypt permissions"",
 ""Effect"": ""Allow"",
 ""Principal"": {
 ""AWS"": ""arn:aws:iam::aws-account-id:user/username""
 },
 ""Action"": ""kms:Decrypt"",
 ""Resource"": ""*"",
 ""Condition"": {
 ""Null"": {
 ""kms:EncryptionContext:aws:cloudtrail:arn"": ""false""
 }
 }
}
```","TITLE:Maintenance, Monitoring and Analysis of Audit Logs CONTROL:v7 6 DESCRIPTION:Maintenance, Monitoring and Analysis of Audit Logs;",CCE-78919-8,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html:https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html,,,,,,,,,
3,3.8,Ensure rotation for customer created CMKs is enabled,published,Automated,AWS Key Management Service (KMS) allows customers to rotate the backing key which is key material stored within the KMS which is tied to the key ID of the Customer Created customer master key (CMK). It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. It is recommended that CMK key rotation be enabled.,Rotating encryption keys helps reduce the potential impact of a compromised key as data encrypted with a new key cannot be accessed with a previous key that may have been exposed.,"**From Console:**

1. Sign in to the AWS Management Console and open the IAM console at [https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam).
2. In the left navigation pane, choose `Encryption Keys` .
3. Select a customer created master key (CMK)
4. Under the `Key Policy` section, move down to `Key Rotation` _._
5. Check the `Rotate this key every year` checkbox.

**From Command Line:**

1. Run the following command to enable key rotation:
```
 aws kms enable-key-rotation --key-id <kms_key_id>
```","**From Console:**

1. Sign in to the AWS Management Console and open the IAM console at [https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam).
2. In the left navigation pane, choose `Encryption Keys` .
3. Select a customer created master key (CMK)
4. Under the `Key Policy` section, move down to `Key Rotation` _._
5. Ensure the `Rotate this key every year` checkbox is checked.

**From Command Line:**

1. Run the following command to get a list of all keys and their associated `KeyIds` 
```
 aws kms list-keys
```
2. For each key, note the KeyId and run the following command
```
 aws kms get-key-rotation-status --key-id <kms_key_id>
```
3. Ensure `KeyRotationEnabled` is set to `true`",,,"TITLE:Maintenance, Monitoring and Analysis of Audit Logs CONTROL:v7 6 DESCRIPTION:Maintenance, Monitoring and Analysis of Audit Logs;",CCE-78920-6,https://aws.amazon.com/kms/pricing/:https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final,,,,,,,,,
3,3.9,Ensure VPC flow logging is enabled in all VPCs,published,Automated,"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. It is recommended that VPC Flow Logs be enabled for packet ""Rejects"" for VPCs.",VPC Flow Logs provide visibility into network traffic that traverses the VPC and can be used to detect anomalous traffic or insight during security workflows.,"Perform the following to determine if VPC Flow logs is enabled:

**From Console:**

1. Sign into the management console
2. Select `Services` then `VPC` 
3. In the left navigation pane, select `Your VPCs` 
4. Select a VPC
5. In the right pane, select the `Flow Logs` tab.
6. If no Flow Log exists, click `Create Flow Log` 
7. For Filter, select `Reject`
8. Enter in a `Role` and `Destination Log Group` 
9. Click `Create Log Flow` 
10. Click on `CloudWatch Logs Group` 

**Note:** Setting the filter to ""Reject"" will dramatically reduce the logging data accumulation for this recommendation and provide sufficient information for the purposes of breach detection, research and remediation. However, during periods of least privilege security group engineering, setting this the filter to ""All"" can be very helpful in discovering existing traffic flows required for proper operation of an already running environment.","Perform the following to determine if VPC Flow logs is enabled:

**From Console:**

1. Sign into the management console
2. Select `Services` then `VPC` 
3. In the left navigation pane, select `Your VPCs` 
4. Select a VPC
5. In the right pane, select the `Flow Logs` tab.
6. Ensure a Log Flow exists that has `Active` in the `Status` column.","By default, CloudWatch Logs will store Logs indefinitely unless a specific retention period is defined for the log group. When choosing the number of days to retain, keep in mind the average days it takes an organization to realize they have been breached is 210 days (at the time of this writing). Since additional time is required to research a breach, a minimum 365 day retention policy allows time for detection and research. You may also wish to archive the logs to a cheaper storage service rather than simply deleting them. See the following AWS resource to manage CloudWatch Logs retention periods:

1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/SettingLogRetention.html",,TITLE:Configure Monitoring Systems to Record Network Packets CONTROL:v7 12.5 DESCRIPTION:Configure monitoring systems to record network packets passing through the boundary at each of the organization's network boundaries.;TITLE:Activate audit logging CONTROL:v7 6.2 DESCRIPTION:Ensure that local logging has been enabled on all systems and networking devices.;,CCE-79202-8,https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html,,,,,,,,,
3,3.10,Ensure that Object-level logging for write events is enabled for S3 bucket,published,Automated,"S3 object-level API operations such as GetObject, DeleteObject, and PutObject are called data events. By default, CloudTrail trails don't log data events and so it is recommended to enable Object-level logging for S3 buckets.","Enabling object-level logging will help you meet data compliance requirements within your organization, perform comprehensive security analysis, monitor specific patterns of user behavior in your AWS account or take immediate actions on any object-level API activity within your S3 Buckets using Amazon CloudWatch Events.","**From Console:**

1. Login to the AWS Management Console and navigate to S3 dashboard at `https://console.aws.amazon.com/s3/`
2. In the left navigation panel, click `buckets` and then click on the S3 Bucket Name that you want to examine.
3. Click `Properties` tab to see in detail bucket configuration.
4. Click on the `Object-level` logging setting, enter the CloudTrail name for the recording activity. You can choose an existing Cloudtrail or create a new one by navigating to the Cloudtrail console link `https://console.aws.amazon.com/cloudtrail/`
5. Once the Cloudtrail is selected, check the `Write` event checkbox, so that `object-level` logging for Write events is enabled.
6. Repeat steps 2 to 5 to enable object-level logging of write events for other S3 buckets.

**From Command Line:**

1. To enable `object-level` data events logging for S3 buckets within your AWS account, run `put-event-selectors` command using the name of the trail that you want to reconfigure as identifier:
```
aws cloudtrail put-event-selectors --region <region-name> --trail-name <trail-name> --event-selectors '[{ ""ReadWriteType"": ""WriteOnly"", ""IncludeManagementEvents"":true, ""DataResources"": [{ ""Type"": ""AWS::S3::Object"", ""Values"": [""arn:aws:s3:::<s3-bucket-name>/""] }] }]'
```
2. The command output will be `object-level` event trail configuration.
3. If you want to enable it for all buckets at once then change Values parameter to `[""arn:aws:s3""]` in command given above.
4. Repeat step 1 for each s3 bucket to update `object-level` logging of write events.
5. Change the AWS region by updating the `--region` command parameter and perform the process for other regions.","**From Console:**

1. Login to the AWS Management Console and navigate to S3 dashboard at `https://console.aws.amazon.com/s3/`
2. In the left navigation panel, click `buckets` and then click on the S3 Bucket Name that you want to examine.
3. Click `Properties` tab to see in detail bucket configuration.
4. If the current status for `Object-level` logging is set to Disabled, then object-level logging of write events for the selected s3 bucket is not set.
5. Repeat steps 2 to 4 to verify object level logging status of other S3 buckets.

**From Command Line:**

1. Run `list-trails` command to list the names of all Amazon CloudTrail trails currently available in the selected AWS region:
```
aws cloudtrail list-trails --region <region-name> --query Trails[*].Name
```
2. The command output will be a list of the requested trail names.
3. Run `get-event-selectors` command using the name of the trail returned at the previous step and custom query filters to determine if Data events logging feature is enabled within the selected CloudTrail trail configuration for s3bucket resources:
```
aws cloudtrail get-event-selectors --region <region-name> --trail-name <trail-name> --query EventSelectors[*].DataResources[]
```
4. The command output should be an array that contains the configuration of the AWS resource(S3 bucket) defined for the Data events selector.
5. If the `get-event-selectors` command returns an empty array '[]', the Data events are not included into the selected AWS Cloudtrail trail logging configuration, therefore the S3 object-level API operations performed within your AWS account are not recorded.
6. Repeat steps 1 to 5 for auditing each s3 bucket to identify other trails that are missing the capability to log Data events.
7. Change the AWS region by updating the `--region` command parameter and perform the audit process for other regions.",,,"TITLE:Activate audit logging CONTROL:v7 6.2 DESCRIPTION:Ensure that local logging has been enabled on all systems and networking devices.;TITLE:Enable Detailed Logging CONTROL:v7 6.3 DESCRIPTION:Enable system logging to include detailed information such as an event source, date, user, timestamp, source addresses, destination addresses, and other useful elements.;",,https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html,,,,,,,,,
3,3.11,Ensure that Object-level logging for read events is enabled for S3 bucket,published,Automated,"S3 object-level API operations such as GetObject, DeleteObject, and PutObject are called data events. By default, CloudTrail trails don't log data events and so it is recommended to enable Object-level logging for S3 buckets.","Enabling object-level logging will help you meet data compliance requirements within your organization, perform comprehensive security analysis, monitor specific patterns of user behavior in your AWS account or take immediate actions on any object-level API activity using Amazon CloudWatch Events.","**From Console:**

1. Login to the AWS Management Console and navigate to S3 dashboard at `https://console.aws.amazon.com/s3/`
2. In the left navigation panel, click `buckets` and then click on the S3 Bucket Name that you want to examine.
3. Click `Properties` tab to see in detail bucket configuration.
4. Click on the `Object-level` logging setting, enter the CloudTrail name for the recording activity. You can choose an existing Cloudtrail or create a new one by navigating to the Cloudtrail console link `https://console.aws.amazon.com/cloudtrail/`
5. Once the Cloudtrail is selected, check the Read event checkbox, so that `object-level` logging for `Read` events is enabled.
6. Repeat steps 2 to 5 to enable `object-level` logging of read events for other S3 buckets.

**From Command Line:**
1. To enable `object-level` data events logging for S3 buckets within your AWS account, run `put-event-selectors` command using the name of the trail that you want to reconfigure as identifier:
```
aws cloudtrail put-event-selectors --region <region-name> --trail-name <trail-name> --event-selectors '[{ ""ReadWriteType"": ""ReadOnly"", ""IncludeManagementEvents"":true, ""DataResources"": [{ ""Type"": ""AWS::S3::Object"", ""Values"": [""arn:aws:s3:::<s3-bucket-name>/""] }] }]'
```
2. The command output will be `object-level` event trail configuration.
3. If you want to enable it for all buckets at ones then change Values parameter to `[""arn:aws:s3""]` in command given above.
4. Repeat step 1 for each s3 bucket to update `object-level` logging of read events.
5. Change the AWS region by updating the `--region` command parameter and perform the process for other regions.","**From Console:**

1. Login to the AWS Management Console and navigate to S3 dashboard at `https://console.aws.amazon.com/s3/`
2. In the left navigation panel, click `buckets` and then click on the S3 Bucket Name that you want to examine.
3. Click `Properties` tab to see in detail bucket configuration.
4. If the current status for `Object-level` logging is set to `Disabled`, then object-level logging of read events for the selected s3 bucket is not set.
5. If the current status for `Object-level` logging is set to `Enabled`, but the Read event check-box is unchecked, then object-level logging of read events for the selected s3 bucket is not set.
6. Repeat steps 2 to 5 to verify `object-level` logging for `read` events of your other S3 buckets.

**From Command Line:**
1. Run `describe-trails` command to list the names of all Amazon CloudTrail trails currently available in the selected AWS region:
```
aws cloudtrail describe-trails --region <region-name> --output table --query trailList[*].Name
```
2. The command output will be table of the requested trail names.
3. Run `get-event-selectors` command using the name of the trail returned at the previous step and custom query filters to determine if Data events logging feature is enabled within the selected CloudTrail trail configuration for s3 bucket resources:
```
aws cloudtrail get-event-selectors --region <region-name> --trail-name <trail-name> --query EventSelectors[*].DataResources[]
```
4. The command output should be an array that contains the configuration of the AWS resource(S3 bucket) defined for the Data events selector.
5. If the `get-event-selectors` command returns an empty array, the Data events are not included into the selected AWS Cloudtrail trail logging configuration, therefore the S3 object-level API operations performed within your AWS account are not recorded.
6. Repeat steps 1 to 5 for auditing each s3 bucket to identify other trails that are missing the capability to log Data events.
7. Change the AWS region by updating the `--region` command parameter and perform the audit process for other regions.",,,"TITLE:Activate audit logging CONTROL:v7 6.2 DESCRIPTION:Ensure that local logging has been enabled on all systems and networking devices.;TITLE:Enable Detailed Logging CONTROL:v7 6.3 DESCRIPTION:Enable system logging to include detailed information such as an event source, date, user, timestamp, source addresses, destination addresses, and other useful elements.;",,https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html,,,,,,,,,
4,,Monitoring,published,,"For effectiveness and coverage of recommended metric-filters and alarms, recommendations in Section 3 should be implemented on Multi-region CloudTrail referred in `Ensure CloudTrail is enabled in all regions`
Updated Overview should look like:
This section contains recommendations for configuring AWS to assist with monitoring and responding to account activities. Metric filter-related recommendations in this section are dependent on the `Ensure CloudTrail is enabled in all regions` and `Ensure CloudTrail trails are integrated with CloudWatch Logs` recommendation in the ""Logging"" section. Additionally, step 3 of the remediation procedure for the same recommendations provides guidance for establishing an email-based subscription (`--protocol email`). This is provided as an example and is not meant to suggest other protocols provide lesser value.",,,,,,,,,,,,,,,,,
4,4.6,Ensure a log metric filter and alarm exist for AWS Management Console authentication failures,published,Automated,Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for failed console authentication attempts.,"Monitoring failed console logins may decrease lead time to detect an attempt to brute force a credential, which may provide an indicator, such as source IP, that can be used in other event correlation.","Perform the following to setup the metric filter, alarm, SNS topic, and subscription:

1. Create a metric filter based on filter pattern provided which checks for AWS management Console Login Failures and the `<cloudtrail_log_group_name>` taken from audit step 1.
```
aws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<console_signin_failure_metric>` --metric-transformations metricName= `<console_signin_failure_metric>` ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = ConsoleLogin) && ($.errorMessage = ""Failed authentication"") }'
```
**Note**: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together.

2. Create an SNS topic that the alarm will notify
```
aws sns create-topic --name <sns_topic_name>
```
**Note**: you can execute this command once and then re-use the same topic for all monitoring alarms.

3. Create an SNS subscription to the topic created in step 2
```
aws sns subscribe --topic-arn <sns_topic_arn> --protocol <protocol_for_sns> --notification-endpoint <sns_subscription_endpoints>
```
**Note**: you can execute this command once and then re-use the SNS subscription for all monitoring alarms.

4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2
```
aws cloudwatch put-metric-alarm --alarm-name `<console_signin_failure_alarm>` --metric-name `<console_signin_failure_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>
```","Perform the following to ensure that there is at least one active multi-region CloudTrail with prescribed metric filters and alarms configured:

1. Identify the log group name configured for use with active multi-region CloudTrail:

- List all CloudTrails: `aws cloudtrail describe-trails`

- Identify Multi region Cloudtrails: `Trails with ""IsMultiRegionTrail"" set to true`

- From value associated with CloudWatchLogsLogGroupArn note `<cloudtrail_log_group_name>`

Example: for CloudWatchLogsLogGroupArn that looks like `arn:aws:logs:<region>:<aws_account_number>:log-group:NewGroup:*`, `<cloudtrail_log_group_name>` would be `NewGroup`

- Ensure Identified Multi region CloudTrail is active

`aws cloudtrail get-trail-status --name <Name of a Multi-region CloudTrail>`

ensure `IsLogging` is set to `TRUE`

- Ensure identified Multi-region Cloudtrail captures all Management Events

`aws cloudtrail get-event-selectors --trail-name <trailname shown in describe-trails>`

Ensure there is at least one Event Selector for a Trail with `IncludeManagementEvents` set to `true` and `ReadWriteType` set to `All`

2. Get a list of all associated metric filters for this `<cloudtrail_log_group_name>`:
```
aws logs describe-metric-filters --log-group-name ""<cloudtrail_log_group_name>""
```
3. Ensure the output from the above command contains the following:
```
""filterPattern"": ""{ ($.eventName = ConsoleLogin) && ($.errorMessage = ""Failed authentication"") }""
```

4. Note the `<console_signin_failure_metric>` value associated with the `filterPattern` found in step 3.

5. Get a list of CloudWatch alarms and filter on the `<console_signin_failure_metric>` captured in step 4.
```
aws cloudwatch describe-alarms --query 'MetricAlarms[?MetricName== `<console_signin_failure_metric>`]'
```
6. Note the `AlarmActions` value - this will provide the SNS topic ARN value.

7. Ensure there is at least one active subscriber to the SNS topic
```
aws sns list-subscriptions-by-topic --topic-arn <sns_topic_arn> 
```
at least one subscription should have ""SubscriptionArn"" with valid aws ARN.
```
Example of valid ""SubscriptionArn"": ""arn:aws:sns:<region>:<aws_account_number>:<SnsTopicName>:<SubscriptionID>""
```",,"Configuring log metric filter and alarm on Multi-region (global) CloudTrail
- ensures that activities from all regions (used as well as unused) are monitored
- ensures that activities on all supported global services are monitored
- ensures that all management events across all regions are monitored",TITLE:Account Monitoring and Control CONTROL:v7 16 DESCRIPTION:Account Monitoring and Control;,CCE-79191-3,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html:https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html:https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html,,,,,,,,,
4,4.7,Ensure a log metric filter and alarm exist for disabling or scheduled deletion of customer created CMKs,published,Automated,Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for customer created CMKs which have changed state to disabled or scheduled deletion.,Data encrypted with disabled or deleted keys will no longer be accessible.,"Perform the following to setup the metric filter, alarm, SNS topic, and subscription:

1. Create a metric filter based on filter pattern provided which checks for disabled or scheduled for deletion CMK's and the `<cloudtrail_log_group_name>` taken from audit step 1.
```
aws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<disable_or_delete_cmk_changes_metric>` --metric-transformations metricName= `<disable_or_delete_cmk_changes_metric>` ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{($.eventSource = kms.amazonaws.com) && (($.eventName=DisableKey)||($.eventName=ScheduleKeyDeletion)) }'
```
**Note**: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together.

2. Create an SNS topic that the alarm will notify
```
aws sns create-topic --name <sns_topic_name>
```
**Note**: you can execute this command once and then re-use the same topic for all monitoring alarms.

3. Create an SNS subscription to the topic created in step 2
```
aws sns subscribe --topic-arn <sns_topic_arn> --protocol <protocol_for_sns> --notification-endpoint <sns_subscription_endpoints>
```
**Note**: you can execute this command once and then re-use the SNS subscription for all monitoring alarms.

4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2
```
aws cloudwatch put-metric-alarm --alarm-name `<disable_or_delete_cmk_changes_alarm>` --metric-name `<disable_or_delete_cmk_changes_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>
```","Perform the following to ensure that there is at least one active multi-region CloudTrail with prescribed metric filters and alarms configured:

1. Identify the log group name configured for use with active multi-region CloudTrail:

- List all CloudTrails: `aws cloudtrail describe-trails`

- Identify Multi region Cloudtrails: `Trails with ""IsMultiRegionTrail"" set to true`

- From value associated with CloudWatchLogsLogGroupArn note `<cloudtrail_log_group_name>`

Example: for CloudWatchLogsLogGroupArn that looks like `arn:aws:logs:<region>:<aws_account_number>:log-group:NewGroup:*`, `<cloudtrail_log_group_name>` would be `NewGroup`

- Ensure Identified Multi region CloudTrail is active

`aws cloudtrail get-trail-status --name <Name of a Multi-region CloudTrail>`

ensure `IsLogging` is set to `TRUE`

- Ensure identified Multi-region Cloudtrail captures all Management Events

`aws cloudtrail get-event-selectors --trail-name <trailname shown in describe-trails>`

Ensure there is at least one Event Selector for a Trail with `IncludeManagementEvents` set to `true` and `ReadWriteType` set to `All`

2. Get a list of all associated metric filters for this `<cloudtrail_log_group_name>`:
```
aws logs describe-metric-filters --log-group-name ""<cloudtrail_log_group_name>""
```
3. Ensure the output from the above command contains the following:
```
""filterPattern"": ""{($.eventSource = kms.amazonaws.com) && (($.eventName=DisableKey)||($.eventName=ScheduleKeyDeletion)) }""
```
4. Note the `<disable_or_delete_cmk_changes_metric>` value associated with the `filterPattern` found in step 3.

5. Get a list of CloudWatch alarms and filter on the `<disable_or_delete_cmk_changes_metric>` captured in step 4.
```
aws cloudwatch describe-alarms --query 'MetricAlarms[?MetricName== `<disable_or_delete_cmk_changes_metric>`]'
```
6. Note the `AlarmActions` value - this will provide the SNS topic ARN value.

7. Ensure there is at least one active subscriber to the SNS topic
```
aws sns list-subscriptions-by-topic --topic-arn <sns_topic_arn> 
```
at least one subscription should have ""SubscriptionArn"" with valid aws ARN.
```
Example of valid ""SubscriptionArn"": ""arn:aws:sns:<region>:<aws_account_number>:<SnsTopicName>:<SubscriptionID>""
```",,"Configuring log metric filter and alarm on Multi-region (global) CloudTrail
- ensures that activities from all regions (used as well as unused) are monitored
- ensures that activities on all supported global services are monitored
- ensures that all management events across all regions are monitored",TITLE:Account Monitoring and Control CONTROL:v7 16 DESCRIPTION:Account Monitoring and Control;,CCE-79192-1,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html:https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html:https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html,,,,,,,,,
4,4.9,Ensure a log metric filter and alarm exist for AWS Config configuration changes,published,Automated,Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for detecting changes to CloudTrail's configurations.,Monitoring changes to AWS Config configuration will help ensure sustained visibility of configuration items within the AWS account.,"Perform the following to setup the metric filter, alarm, SNS topic, and subscription:

1. Create a metric filter based on filter pattern provided which checks for AWS Configuration changes and the `<cloudtrail_log_group_name>` taken from audit step 1.
```
aws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<aws_config_changes_metric>` --metric-transformations metricName= `<aws_config_changes_metric>` ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = config.amazonaws.com) && (($.eventName=StopConfigurationRecorder)||($.eventName=DeleteDeliveryChannel)||($.eventName=PutDeliveryChannel)||($.eventName=PutConfigurationRecorder)) }'
```

**Note**: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together.

2. Create an SNS topic that the alarm will notify
```
aws sns create-topic --name <sns_topic_name>
```

**Note**: you can execute this command once and then re-use the same topic for all monitoring alarms.

3. Create an SNS subscription to topic created in step 2
```
aws sns subscribe --topic-arn <sns_topic_arn> --protocol <protocol_for_sns> --notification-endpoint <sns_subscription_endpoints>
```

**Note**: you can execute this command once and then re-use the SNS subscription for all monitoring alarms.

4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2
```
aws cloudwatch put-metric-alarm --alarm-name `<aws_config_changes_alarm>` --metric-name `<aws_config_changes_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>
```","Perform the following to ensure that there is at least one active multi-region CloudTrail with prescribed metric filters and alarms configured:

1. Identify the log group name configured for use with active multi-region CloudTrail:

- List all CloudTrails: `aws cloudtrail describe-trails`

- Identify Multi region Cloudtrails: `Trails with ""IsMultiRegionTrail"" set to true`

- From value associated with CloudWatchLogsLogGroupArn note `<cloudtrail_log_group_name>`

Example: for CloudWatchLogsLogGroupArn that looks like `arn:aws:logs:<region>:<aws_account_number>:log-group:NewGroup:*`, `<cloudtrail_log_group_name>` would be `NewGroup`

- Ensure Identified Multi region CloudTrail is active

`aws cloudtrail get-trail-status --name <Name of a Multi-region CloudTrail>`

ensure `IsLogging` is set to `TRUE`

- Ensure identified Multi-region Cloudtrail captures all Management Events

`aws cloudtrail get-event-selectors --trail-name <trailname shown in describe-trails>`

Ensure there is at least one Event Selector for a Trail with `IncludeManagementEvents` set to `true` and `ReadWriteType` set to `All`

2. Get a list of all associated metric filters for this `<cloudtrail_log_group_name>`:
```
aws logs describe-metric-filters --log-group-name ""<cloudtrail_log_group_name>""
```
3. Ensure the output from the above command contains the following:
```
""filterPattern"": ""{ ($.eventSource = config.amazonaws.com) && (($.eventName=StopConfigurationRecorder)||($.eventName=DeleteDeliveryChannel)||($.eventName=PutDeliveryChannel)||($.eventName=PutConfigurationRecorder)) }""
```
4. Note the `<aws_config_changes_metric>` value associated with the `filterPattern` found in step 3.

5. Get a list of CloudWatch alarms and filter on the `<aws_config_changes_metric>` captured in step 4.
```
aws cloudwatch describe-alarms --query 'MetricAlarms[?MetricName== `<aws_config_changes_metric>`]'
```
6. Note the `AlarmActions` value - this will provide the SNS topic ARN value.

7. Ensure there is at least one active subscriber to the SNS topic
```
aws sns list-subscriptions-by-topic --topic-arn <sns_topic_arn> 
```
at least one subscription should have ""SubscriptionArn"" with valid aws ARN.
```
Example of valid ""SubscriptionArn"": ""arn:aws:sns:<region>:<aws_account_number>:<SnsTopicName>:<SubscriptionID>""
```",,"Configuring log metric filter and alarm on Multi-region (global) CloudTrail
- ensures that activities from all regions (used as well as unused) are monitored
- ensures that activities on all supported global services are monitored
- ensures that all management events across all regions are monitored","TITLE:Maintain Detailed Asset Inventory CONTROL:v7 1.4 DESCRIPTION:Maintain an accurate and up-to-date inventory of all technology assets with the potential to store or process information. This inventory shall include all hardware assets, whether connected to the organization's network or not.;TITLE:Document Traffic Configuration Rules CONTROL:v7 11.2 DESCRIPTION:All configuration rules that allow traffic to flow through network devices should be documented in a configuration management system with a specific business reason for each rule, a specific individual’s name responsible for that business need, and an expected duration of the need.;TITLE:Maintain an Inventory of Authentication Systems CONTROL:v7 16.1 DESCRIPTION:Maintain an inventory of each of the organization's authentication systems, including those located onsite or at a remote service provider.;",CCE-79194-7,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html:https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html:https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html,,,,,,,,,
4,4.10,Ensure a log metric filter and alarm exist for security group changes,published,Automated,Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Security Groups are a stateful packet filter that controls ingress and egress traffic within a VPC. It is recommended that a metric filter and alarm be established for detecting changes to Security Groups.,Monitoring changes to security group will help ensure that resources and services are not unintentionally exposed.,"Perform the following to setup the metric filter, alarm, SNS topic, and subscription:

1. Create a metric filter based on filter pattern provided which checks for security groups changes and the `<cloudtrail_log_group_name>` taken from audit step 1.
```
aws logs put-metric-filter --log-group-name ""<cloudtrail_log_group_name>"" --filter-name ""<security_group_changes_metric>"" --metric-transformations metricName= ""<security_group_changes_metric>"" ,metricNamespace=""CISBenchmark"",metricValue=1 --filter-pattern ""{ ($.eventName = AuthorizeSecurityGroupIngress) || ($.eventName = AuthorizeSecurityGroupEgress) || ($.eventName = RevokeSecurityGroupIngress) || ($.eventName = RevokeSecurityGroupEgress) || ($.eventName = CreateSecurityGroup) || ($.eventName = DeleteSecurityGroup) }""
```

**Note**: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together.

2. Create an SNS topic that the alarm will notify
```
aws sns create-topic --name ""<sns_topic_name>""
```

**Note**: you can execute this command once and then re-use the same topic for all monitoring alarms.

3. Create an SNS subscription to the topic created in step 2
```
aws sns subscribe --topic-arn ""<sns_topic_arn>"" --protocol <protocol_for_sns> --notification-endpoint ""<sns_subscription_endpoints>""
```

**Note**: you can execute this command once and then re-use the SNS subscription for all monitoring alarms.

4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2
```
aws cloudwatch put-metric-alarm --alarm-name ""<security_group_changes_alarm>"" --metric-name ""<security_group_changes_metric>"" --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace ""CISBenchmark"" --alarm-actions ""<sns_topic_arn>""
```","Perform the following to ensure that there is at least one active multi-region CloudTrail with prescribed metric filters and alarms configured:

1. Identify the log group name configured for use with active multi-region CloudTrail:

- List all CloudTrails: `aws cloudtrail describe-trails`

- Identify Multi region Cloudtrails: `Trails with ""IsMultiRegionTrail"" set to true`

- From value associated with CloudWatchLogsLogGroupArn note `<cloudtrail_log_group_name>`

Example: for CloudWatchLogsLogGroupArn that looks like `arn:aws:logs:<region>:<aws_account_number>:log-group:NewGroup:*`, `<cloudtrail_log_group_name>` would be `NewGroup`

- Ensure Identified Multi region CloudTrail is active

`aws cloudtrail get-trail-status --name <Name of a Multi-region CloudTrail>`

ensure `IsLogging` is set to `TRUE`

- Ensure identified Multi-region Cloudtrail captures all Management Events

`aws cloudtrail get-event-selectors --trail-name <trailname shown in describe-trails>`

Ensure there is at least one Event Selector for a Trail with `IncludeManagementEvents` set to `true` and `ReadWriteType` set to `All`

2. Get a list of all associated metric filters for this `<cloudtrail_log_group_name>`:
```
aws logs describe-metric-filters --log-group-name ""<cloudtrail_log_group_name>""
```
3. Ensure the output from the above command contains the following:
```
""filterPattern"": ""{ ($.eventName = AuthorizeSecurityGroupIngress) || ($.eventName = AuthorizeSecurityGroupEgress) || ($.eventName = RevokeSecurityGroupIngress) || ($.eventName = RevokeSecurityGroupEgress) || ($.eventName = CreateSecurityGroup) || ($.eventName = DeleteSecurityGroup) }""
```
4. Note the `<security_group_changes_metric>` value associated with the `filterPattern` found in step 3.

5. Get a list of CloudWatch alarms and filter on the `<security_group_changes_metric>` captured in step 4.
```
aws cloudwatch describe-alarms --query ""MetricAlarms[?MetricName== '<security_group_changes_metric>']""
```
6. Note the `AlarmActions` value - this will provide the SNS topic ARN value.

7. Ensure there is at least one active subscriber to the SNS topic
```
aws sns list-subscriptions-by-topic --topic-arn <sns_topic_arn> 
```
at least one subscription should have ""SubscriptionArn"" with valid aws ARN.
```
Example of valid ""SubscriptionArn"": ""arn:aws:sns:<region>:<aws_account_number>:<SnsTopicName>:<SubscriptionID>""
```",,"Configuring log metric filter and alarm on Multi-region (global) CloudTrail
- ensures that activities from all regions (used as well as unused) are monitored
- ensures that activities on all supported global services are monitored
- ensures that all management events across all regions are monitored","TITLE:Activate audit logging CONTROL:v7 6.2 DESCRIPTION:Ensure that local logging has been enabled on all systems and networking devices.;TITLE:Protect Information through Access Control Lists CONTROL:v7 14.6 DESCRIPTION:Protect all information stored on systems with file system, network share, claims, application, or database specific access control lists. These controls will enforce the principle that only authorized individuals should have access to the information based on their need to access the information as a part of their responsibilities.;",CCE-79195-4,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html:https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html:https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html,,,,,,,,,
4,4.11,Ensure a log metric filter and alarm exist for changes to Network Access Control Lists (NACL),published,Automated,Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. NACLs are used as a stateless packet filter to control ingress and egress traffic for subnets within a VPC. It is recommended that a metric filter and alarm be established for changes made to NACLs.,Monitoring changes to NACLs will help ensure that AWS resources and services are not unintentionally exposed.,"Perform the following to setup the metric filter, alarm, SNS topic, and subscription:

1. Create a metric filter based on filter pattern provided which checks for NACL changes and the `<cloudtrail_log_group_name>` taken from audit step 1.
```
aws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<nacl_changes_metric>` --metric-transformations metricName= `<nacl_changes_metric>` ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventName = CreateNetworkAcl) || ($.eventName = CreateNetworkAclEntry) || ($.eventName = DeleteNetworkAcl) || ($.eventName = DeleteNetworkAclEntry) || ($.eventName = ReplaceNetworkAclEntry) || ($.eventName = ReplaceNetworkAclAssociation) }'
```

**Note**: You can choose your own metricName and metricNamespace strings. Using the same metricNamespace for all Foundations Benchmark metrics will group them together.

2. Create an SNS topic that the alarm will notify
```
aws sns create-topic --name <sns_topic_name>
```

**Note**: you can execute this command once and then re-use the same topic for all monitoring alarms.

3. Create an SNS subscription to the topic created in step 2
```
aws sns subscribe --topic-arn <sns_topic_arn> --protocol <protocol_for_sns> --notification-endpoint <sns_subscription_endpoints>
```

**Note**: you can execute this command once and then re-use the SNS subscription for all monitoring alarms.

4. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 1 and an SNS topic created in step 2
```
aws cloudwatch put-metric-alarm --alarm-name `<nacl_changes_alarm>` --metric-name `<nacl_changes_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>
```","Perform the following to ensure that there is at least one active multi-region CloudTrail with prescribed metric filters and alarms configured:

1. Identify the log group name configured for use with active multi-region CloudTrail:

- List all CloudTrails: `aws cloudtrail describe-trails`

- Identify Multi region Cloudtrails: `Trails with ""IsMultiRegionTrail"" set to true`

- From value associated with CloudWatchLogsLogGroupArn note `<cloudtrail_log_group_name>`

Example: for CloudWatchLogsLogGroupArn that looks like `arn:aws:logs:<region>:<aws_account_number>:log-group:NewGroup:*`, `<cloudtrail_log_group_name>` would be `NewGroup`

- Ensure Identified Multi region CloudTrail is active

`aws cloudtrail get-trail-status --name <Name of a Multi-region CloudTrail>`

ensure `IsLogging` is set to `TRUE`

- Ensure identified Multi-region Cloudtrail captures all Management Events

`aws cloudtrail get-event-selectors --trail-name <trailname shown in describe-trails>`

Ensure there is at least one Event Selector for a Trail with `IncludeManagementEvents` set to `true` and `ReadWriteType` set to `All`

2. Get a list of all associated metric filters for this `<cloudtrail_log_group_name>`:
```
aws logs describe-metric-filters --log-group-name ""<cloudtrail_log_group_name>""
```
3. Ensure the output from the above command contains the following:
```
""filterPattern"": ""{ ($.eventName = CreateNetworkAcl) || ($.eventName = CreateNetworkAclEntry) || ($.eventName = DeleteNetworkAcl) || ($.eventName = DeleteNetworkAclEntry) || ($.eventName = ReplaceNetworkAclEntry) || ($.eventName = ReplaceNetworkAclAssociation) }""
```
4. Note the `<nacl_changes_metric>` value associated with the `filterPattern` found in step 3.

5. Get a list of CloudWatch alarms and filter on the `<nacl_changes_metric>` captured in step 4.
```
aws cloudwatch describe-alarms --query 'MetricAlarms[?MetricName== `<nacl_changes_metric>`]'
```
6. Note the `AlarmActions` value - this will provide the SNS topic ARN value.

7. Ensure there is at least one active subscriber to the SNS topic
```
aws sns list-subscriptions-by-topic --topic-arn <sns_topic_arn> 
```
at least one subscription should have ""SubscriptionArn"" with valid aws ARN.
```
Example of valid ""SubscriptionArn"": ""arn:aws:sns:<region>:<aws_account_number>:<SnsTopicName>:<SubscriptionID>""
```",,"Configuring log metric filter and alarm on Multi-region (global) CloudTrail
- ensures that activities from all regions (used as well as unused) are monitored
- ensures that activities on all supported global services are monitored
- ensures that all management events across all regions are monitored",TITLE:Use Automated Tools to Verify Standard Device Configurations and Detect Changes CONTROL:v7 11.3 DESCRIPTION:Compare all network device configuration against approved security configurations defined for each network device in use and alert when any deviations are discovered.;,CCE-79196-2,https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html:https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html:https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html,,,,,,,,,
5,,Networking,published,,This section contains recommendations for configuring security-related aspects of the default Virtual Private Cloud (VPC),,,,,,,,,,,,,,,,,
5,5.3,Ensure the default security group of every VPC restricts all traffic,published,Automated,"A VPC comes with a default security group whose initial settings deny all inbound traffic, allow all outbound traffic, and allow all traffic between instances assigned to the security group. If you don't specify a security group when you launch an instance, the instance is automatically assigned to this default security group. Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that the default security group restrict all traffic.

The default VPC in every region should have its default security group updated to comply. Any newly created VPCs will automatically contain a default security group that will need remediation to comply with this recommendation.

**NOTE:** When implementing this recommendation, VPC flow logging is invaluable in determining the least privilege port access required by systems to work properly because it can log all packet acceptances and rejections occurring under the current security groups. This dramatically reduces the primary barrier to least privilege engineering - discovering the minimum ports required by systems in the environment. Even if the VPC flow logging recommendation in this benchmark is not adopted as a permanent security measure, it should be used during any period of discovery and engineering for least privileged security groups.",Configuring all VPC default security groups to restrict all traffic will encourage least privilege security group development and mindful placement of AWS resources into security groups which will in-turn reduce the exposure of those resources.,"Security Group Members

Perform the following to implement the prescribed state:

1. Identify AWS resources that exist within the default security group
2. Create a set of least privilege security groups for those resources
3. Place the resources in those security groups
4. Remove the resources noted in #1 from the default security group

Security Group State

1. Login to the AWS Management Console at [https://console.aws.amazon.com/vpc/home](https://console.aws.amazon.com/vpc/home)
2. Repeat the next steps for all VPCs - including the default VPC in each AWS region:
3. In the left pane, click `Security Groups` 
4. For each default security group, perform the following:
1. Select the `default` security group
2. Click the `Inbound Rules` tab
3. Remove any inbound rules
4. Click the `Outbound Rules` tab
5. Remove any inbound rules

Recommended:

IAM groups allow you to edit the ""name"" field. After remediating default groups rules for all VPCs in all regions, edit this field to add text similar to ""DO NOT USE. DO NOT ADD RULES""","Perform the following to determine if the account is configured as prescribed:

Security Group State

1. Login to the AWS Management Console at [https://console.aws.amazon.com/vpc/home](https://console.aws.amazon.com/vpc/home)
2. Repeat the next steps for all VPCs - including the default VPC in each AWS region:
3. In the left pane, click `Security Groups` 
4. For each default security group, perform the following:
1. Select the `default` security group
2. Click the `Inbound Rules` tab
3. Ensure no rule exist
4. Click the `Outbound Rules` tab
5. Ensure no rules exist

Security Group Members

1. Login to the AWS Management Console at [https://console.aws.amazon.com/vpc/home](https://console.aws.amazon.com/vpc/home)
2. Repeat the next steps for all default groups in all VPCs - including the default VPC in each AWS region:
3. In the left pane, click `Security Groups` 
4. Copy the id of the default security group.
5. Change to the EC2 Management Console at https://console.aws.amazon.com/ec2/v2/home
6. In the filter column type 'Security Group ID : < security group id from #4 >'",Implementing this recommendation in an existing VPC containing operating resources requires extremely careful migration planning as the default security groups are likely to be enabling many ports that are unknown. Enabling VPC flow logging (of accepts) in an existing environment that is known to be breach free will reveal the current pattern of ports being used for each instance to communicate successfully.,,"TITLE:Protect Information through Access Control Lists CONTROL:v7 14.6 DESCRIPTION:Protect all information stored on systems with file system, network share, claims, application, or database specific access control lists. These controls will enforce the principle that only authorized individuals should have access to the information based on their need to access the information as a part of their responsibilities.;",CCE-79201-0,https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html#default-security-group,,,,,,,,,
5,5.4,"Ensure routing tables for VPC peering are ""least access""",published,Manual,"Once a VPC peering connection is established, routing tables must be updated to establish any connections between the peered VPCs. These routes can be as specific as desired - even peering a VPC to only a single host on the other side of the connection.",Being highly selective in peering routing tables is a very effective way of minimizing the impact of breach as resources outside of these routes are inaccessible to the peered VPC.,"Remove and add route table entries to ensure that the least number of subnets or hosts as is required to accomplish the purpose for peering are routable.

**From Command Line:**

1. For each _<route\_table\_id>_ containing routes non compliant with your routing policy (which grants more than desired ""least access""), delete the non compliant route:
```
aws ec2 delete-route --route-table-id <route_table_id> --destination-cidr-block <non_compliant_destination_CIDR>
```
 2. Create a new compliant route:
```
aws ec2 create-route --route-table-id <route_table_id> --destination-cidr-block <compliant_destination_CIDR> --vpc-peering-connection-id <peering_connection_id>
```","Review routing tables of peered VPCs for whether they route all subnets of each VPC and whether that is necessary to accomplish the intended purposes for peering the VPCs.

**From Command Line:**

1. List all the route tables from a VPC and check if ""GatewayId"" is pointing to a _<peering\_connection\_id>_ (e.g. pcx-1a2b3c4d) and if ""DestinationCidrBlock"" is as specific as desired.
```
aws ec2 describe-route-tables --filter ""Name=vpc-id,Values=<vpc_id>"" --query ""RouteTables[*].{RouteTableId:RouteTableId, VpcId:VpcId, Routes:Routes, AssociatedSubnets:Associations[*].SubnetId}""
```",,"If an organization has AWS transit gateway implemented in their VPC architecture they should look to apply the recommendation above for ""least access"" routing architecture at the AWS transit gateway level in combination with what must be implemented at the standard VPC route table. More specifically, to route traffic between two or more VPCs via a transit gateway VPCs must have an attachment to a transit gateway route table as well as a route, therefore to avoid routing traffic between VPCs an attachment to the transit gateway route table should only be added where there is an intention to route traffic between the VPCs. As transit gateways are able to host multiple route tables it is possible to group VPCs by attaching them to a common route table.","TITLE:Protect Information through Access Control Lists CONTROL:v7 14.6 DESCRIPTION:Protect all information stored on systems with file system, network share, claims, application, or database specific access control lists. These controls will enforce the principle that only authorized individuals should have access to the information based on their need to access the information as a part of their responsibilities.;",,https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/peering-configurations-partial-access.html:https://docs.aws.amazon.com/cli/latest/reference/ec2/create-vpc-peering-connection.html,,,,,,,,,
